#- name: Create service account for HDFS
#  user: name={{ hadoop.user }}
#        system=yes
#        shell={{ hadoop.user_shell }}
#        state=present
#        groups="{{ hadoop.user_groups | join(',') }}"

- name: ensure tar is present
  package:
    name: "{{ item }}"
    state: present
  with_items:
  - tar
  - unzip
  become: yes

- name: define number of nodes
  set_fact: number_of_nodes=1 #"{{ groups['nodes'] | length | int }}"

- debug:
     msg: "Number of data nodes: {{ number_of_nodes }}"

- name: disable hdfs replication in single node
  set_fact: hdfs_replication_factor=1
  when: number_of_nodes | int < 3

- name: enable hdfs replication when cluster has more then three nodes
  set_fact: hdfs_replication_factor=3
  when: number_of_nodes | int >= 3

- debug:
     msg: "HDFS replication factor: {{ hdfs_replication_factor }}"

- name: remove pre-existent hadoop data directory
  file:
    path: "{{ hadoop.data_dir }}/"
    state: absent

- name: create hadoop data directory
  file:
    path: "{{ hadoop.data_dir }}"
    state: directory
#    owner: "{{ hadoop.user }}"
#    group: "{{ hadoop.user }}"
#  become: true

- name: remove pre-existent hadoop name directory
  file:
    path: "{{ hadoop.name_dir }}/"
    state: absent

- name: create hadoop name directory
  file:
    path: "{{ hadoop.name_dir }}"
    state: directory
#    owner: "{{ hadoop.user }}"
#    group: "{{ hadoop.user }}"
#  become: true


- name: remove pre-existent hadoop temp directory
  file:
    path: "{{ hadoop.temp_dir }}/"
    state: absent

- name: create hadoop temp directory
  file:
    path: "{{ hadoop.temp_dir }}"
    state: directory
#    owner: "{{ hadoop.user }}"
#    group: "{{ hadoop.user }}"
#  become: true
#
- name: create install temp directory
  file:
    path: "{{ install_temp_dir }}"
    state: directory
 
- name: create install directory
  file:
    path: "{{ hadoop_install_dir }}"
    state: directory
#    owner: "{{ hadoop.user }}"
#    group: "{{ hadoop.user }}"
#  become: true

- name: download hadoop
  get_url:
    url: "{{ hadoop.download_location }}/hadoop-{{ hadoop.version }}/{{ hadoop.hadoop_archive }}"
    dest: "/tmp/{{ hadoop.hadoop_archive }}"

- name: unarchive to the install directory
  unarchive:
    src: "/tmp/{{ hadoop.hadoop_archive }}"
    dest: "{{ hadoop_install_dir }}"
    remote_src: true
    extra_opts:
    - "--strip-components=1"

- name: set core-site.xml
  template:
    src: "core-site.xml.j2"
    dest: "{{ hadoop_install_dir }}/etc/hadoop/core-site.xml"

- name: set hadoop-env.sh
  template:
    src: "hadoop-env.sh.j2"
    dest: "{{ hadoop_install_dir }}/etc/hadoop/hadoop-env.sh"

- name: set hdfs-site.xml
  template:
    src: "hdfs-site.xml.j2"
    dest: "{{ hadoop_install_dir }}/etc/hadoop/hdfs-site.xml"

- name: set slaves
  template:
    src: "slaves.j2"
    dest: "{{ hadoop_install_dir }}/etc/hadoop/slaves"
  when: number_of_nodes | int >= 3
  become: true

# Environment setup.
- name: add hadoop profile to startup
  template:
    src: hadoop-profile.sh.j2
    dest: /etc/profile.d/hadoop-profile.sh
    mode: 0644

- name: format hdfs
  shell: ". /etc/profile.d/java_home.sh; bin/hdfs namenode -format"
  args:
    chdir: "{{ hadoop_install_dir }}"
      #when: (inventory_hostname in groups['master'])
